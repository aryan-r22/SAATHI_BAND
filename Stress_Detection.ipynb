{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "SnFNnnfDTuoJ"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "\n",
    "import sklearn\n",
    "from  sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.models import load_model\n",
    "\n",
    "class DataManager:\n",
    "    # Path to the WESAD dataset\n",
    "    ROOT_PATH = '/content/drive/MyDrive/Colab Notebooks/WESAD'\n",
    "    #ROOT_PATH = r'C:\\WESAD'\n",
    "    \n",
    "    # pickle file extension for importing\n",
    "    FILE_EXT = '.pkl'\n",
    "\n",
    "    # Directory in project structure where model files are stored\n",
    "    MODELS_DIR = os.path.join(Path().absolute(), 'src', 'models')\n",
    "\n",
    "    # IDs of the subjects\n",
    "    SUBJECTS = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "    \n",
    "    # Label values defined in the WESAD readme\n",
    "    BASELINE = 1\n",
    "    STRESS = 2\n",
    "    \n",
    "    FEATURE_KEYS =     ['max',  'min', 'mean', 'range', 'std']\n",
    "    FEATURE_ACC_KEYS = ['maxx', 'maxy', 'maxz', 'mean', 'std']\n",
    "\n",
    "    # Keys for measurements collected by the RespiBAN on the chest\n",
    "    # minus the ones we don't want\n",
    "    # RAW_SENSOR_VALUES = ['ACC','ECG','EDA','EMG','Resp','Temp']\n",
    "    RAW_SENSOR_VALUES_C = ['ECG']     ##Added ECG and EMG \n",
    "    RAW_SENSOR_VALUES_W = ['ACC','BVP', 'TEMP','EDA']\n",
    "    FEATURES_C = {\n",
    "                'ecg_max': [], 'ecg_min': [], 'ecg_mean' : [] , 'ecg_range' : [], 'ecg_std' : [], \\\n",
    "                }\n",
    "\n",
    "    FEATURES_W = {'a_mean': [], 'a_std': [], 'a_maxx': [], 'a_maxy': [], 'a_maxz': [], \\\n",
    "                  'b_mean': [], 'b_std': [], 'b_maxx': [], 'b_maxy': [], 'b_maxz': [], \\\n",
    "                  't_max': [],  't_min': [], 't_mean': [], 't_range': [], 't_std': [], \\\n",
    "                  'e_max': [],  'e_min': [], 'e_mean': [], 'e_range': [], 'e_std': [], \\\n",
    "                } \n",
    "\n",
    "    FEATURES = {\n",
    "                  'ecg_max': [], 'ecg_min': [], 'ecg_mean' : [] , 'ecg_range' : [], 'ecg_std' : [], \\\n",
    "                  'a_mean': [], 'a_std': [], 'a_maxx': [], 'a_maxy': [], 'a_maxz': [], \\\n",
    "                  'b_mean': [], 'b_std': [], 'b_max': [], 'b_range': [], 'b_min': [], \\\n",
    "                  't_max': [],  't_min': [], 't_mean': [], 't_range': [], 't_std': [], \\\n",
    "                  'e_max': [],  'e_min': [], 'e_mean': [], 'e_range': [], 'e_std': [], \\\n",
    "        \n",
    "                }\n",
    "\n",
    "                # Check \n",
    "    STRESS_FEATURES = {\n",
    "                  'ecg_max': [], 'ecg_min': [], 'ecg_mean' : [] , 'ecg_range' : [], 'ecg_std' : [], \\\n",
    "                  'a_mean': [], 'a_std': [], 'a_maxx': [], 'a_maxy': [], 'a_maxz': [], \\\n",
    "                  'b_mean': [], 'b_std': [], 'b_max': [], 'b_range': [], 'b_min': [], \\\n",
    "                  't_max': [],  't_min': [], 't_mean': [], 't_range': [], 't_std': [], \\\n",
    "                  'e_max': [],  'e_min': [], 'e_mean': [], 'e_range': [], 'e_std': [], \\\n",
    "        \n",
    "                }\n",
    "    \n",
    "    # Dictionaries to store the two sets of data\n",
    "    BASELINE_DATA = []\n",
    "    STRESS_DATA = []\n",
    "    \n",
    "    # the file name for the last created model\n",
    "    last_saved=''\n",
    "    \n",
    "    def __init__(self, ignore_empatica=True, ignore_additional_signals=True):         #dont know what empatica means\n",
    "        # denotes that we will be excluding the empatica data \n",
    "        # after loading those measurements\n",
    "        self.ignore_empatica = ignore_empatica\n",
    "\n",
    "    def get_subject_path(self, subject):\n",
    "        \"\"\" \n",
    "        Parameters:\n",
    "        subject (int): id of the subject\n",
    "        \n",
    "        Returns:\n",
    "        str: path to the pickle file for the given subject number\n",
    "             iff the path exists \n",
    "        \"\"\"\n",
    "        \n",
    "        # subjects path looks like data_set + '<subject>/<subject>.pkl'\n",
    "\n",
    "        path = DataManager.ROOT_PATH + '/S' + str(subject) + '/S' + str(subject) + DataManager.FILE_EXT \n",
    "\n",
    "        #path = os.path.join(DataManager.ROOT_PATH, 'S'+ str(subject), 'S' + str(subject) + DataManager.FILE_EXT)\n",
    "        \n",
    "        print('Loading data for S'+ str(subject))\n",
    "        return path\n",
    "        #print('Path=' + path)\n",
    "        #if os.path.isfile(path):\n",
    "         #   return path\n",
    "        #else:\n",
    "         #   print(path)\n",
    "          #  raise Exception('Invalid subject: ' + str(subject))\n",
    "\n",
    "    def load(self, subject):\n",
    "        \"\"\" \n",
    "        Loads and saves the data from the pkl file for the provided subject\n",
    "        \n",
    "        Parameters:\n",
    "        subject (int): id of the subject\n",
    "        \n",
    "        Returns: Baseline and stress data\n",
    "        dict: {{'EDA': [###, ..], ..}, \n",
    "               {'EDA': [###, ..], ..} }\n",
    "        \"\"\"\n",
    "       \n",
    "        # change the encoding because the data appears to have been\n",
    "        # pickled with py2 and we are in py3\n",
    "        with open(self.get_subject_path(subject), 'rb') as file:\n",
    "            data = pickle.load(file, encoding='latin1')\n",
    "            return self.extract_and_reform(data, subject)\n",
    "    \n",
    "    def load_all(self, subjects=SUBJECTS):\n",
    "        \"\"\" \n",
    "        Parameters:\n",
    "        subjects (list): subject ids to be loaded\n",
    "        \n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        for subject in subjects:\n",
    "            self.load(subject)\n",
    "                \n",
    "    \n",
    "    def extract_and_reform(self, data, subject):\n",
    "        \"\"\" \n",
    "        Extracts and shapes the data from the pkl file\n",
    "        for the provided subject.\n",
    "        \n",
    "        Parameters:\n",
    "        data (dict): as loaded from the pickle file\n",
    "        \n",
    "        Returns: Baseline and stress data\n",
    "        dict: {{'EDA': [###, ..], ..}, \n",
    "               {'EDA': [###, ..], ..} }\n",
    "        \"\"\"\n",
    "                \n",
    "        #if self.ignore_empatica:\n",
    "         #   del data['signal']['wrist']\n",
    "        \n",
    "        baseline_indices = np.nonzero(data['label']==DataManager.BASELINE)[0]   \n",
    "        stress_indices = np.nonzero(data['label']==DataManager.STRESS)[0]\n",
    "        base = dict()\n",
    "        stress = dict()\n",
    "        \n",
    "        for value in DataManager.RAW_SENSOR_VALUES_C: \n",
    "            base[value] = data['signal']['chest'][value][baseline_indices]\n",
    "            stress[value] = data['signal']['chest'][value][stress_indices]\n",
    "\n",
    "        for value in DataManager.RAW_SENSOR_VALUES_W:\n",
    "          #a=np.ones(len(s2_data['signal']['wrist']['ACC']))*round(700/32)\n",
    "            mult = 1\n",
    "            y = np.ones(len(data['signal']['wrist'][value]))\n",
    "\n",
    "            if(value == 'ACC'):\n",
    "              mult = 700/32\n",
    "            if (value == 'TEMP'):\n",
    "              mult = 700/4\n",
    "            if (value == 'BVP'):\n",
    "              mult = 700/64\n",
    "            if (value == 'EDA') :\n",
    "              mult = 700/4\n",
    "\n",
    "            y = y*round(mult)\n",
    "            y = y.astype(int)\n",
    "            y = y.tolist()\n",
    "\n",
    "            x = np.repeat(data['signal']['wrist'][value],y,axis=0)\n",
    "            \n",
    "            x1 = x[baseline_indices]\n",
    "            x2 = x[stress_indices]\n",
    "            \n",
    "            #base[value] = data['signal']['wrist'][value][baseline_indices]\n",
    "            #stress[value] = data['signal']['wrist'][value][stress_indices] \n",
    "\n",
    "            base[value] = x1\n",
    "            stress[value] = x2 \n",
    "\n",
    "        DataManager.BASELINE_DATA.append(base)\n",
    "        DataManager.STRESS_DATA.append(stress)\n",
    "        \n",
    "        return base, stress\n",
    "    \n",
    "    def get_stats(self, values, window_size=42000, window_shift=175):\n",
    "        \"\"\" \n",
    "        Calculates basic statistics including max, min, mean, and std\n",
    "        for the given data\n",
    "        \n",
    "        Parameters:\n",
    "        values (numpy.ndarray): list of numeric sensor values\n",
    "        \n",
    "        Returns: \n",
    "        dict: \n",
    "        \"\"\"\n",
    "#        print(\"There are \",  values.size, \" samples being considered.\")\n",
    "        num_features = values.size - window_size\n",
    "#        print(\"Computing \", num_features , \" feature values with window size\" \\\n",
    "#              \"of \", str(window_size) + \".\" )        \n",
    "        max_tmp = []\n",
    "        min_tmp = []\n",
    "        mean_tmp = []\n",
    "        dynamic_range_tmp = []\n",
    "        std_tmp = []\n",
    "        for i in range(0, num_features, window_shift):\n",
    "            window = values[i:window_size + i]\n",
    "            max_tmp.append(np.amax(window))\n",
    "            min_tmp.append(np.amin(window))\n",
    "            mean_tmp.append(np.mean(window))\n",
    "            dynamic_range_tmp.append(max_tmp[-1] - min_tmp[-1])\n",
    "            std_tmp.append(np.std(window))\n",
    "\n",
    "        features = {}\n",
    "        features['max'] = max_tmp\n",
    "        features['min'] = min_tmp\n",
    "        features['mean'] = mean_tmp\n",
    "        features['range'] = dynamic_range_tmp\n",
    "        features['std'] = std_tmp\n",
    "        return features\n",
    "\n",
    "    def get_features_for_acc(self, values, window_size=42000, window_shift=175):\n",
    "        \"\"\" \n",
    "        Calculates statistics including mean and std\n",
    "        for the given data and the peak frequency per axis and the\n",
    "        body acceleration component (RMS)\n",
    "        \n",
    "        Parameters:\n",
    "        values (numpy.ndarray): list of numeric sensor values [x, y, z]\n",
    "        window_size (int): specifies size of the sliding window\n",
    "        window_shift (int): Specifies the sliding window shift\n",
    "        \n",
    "        Returns: \n",
    "        dict: features for mean, max, std for given data\n",
    "        \"\"\"\n",
    "        #print(\"There are \", len(values[:,1]), \" samples being considered.\")\n",
    "        num_features = len(values[:,1]) - window_size\n",
    "        #print(\"Computing \", num_features , \" feature values with window size\" \\\n",
    "        #              \"of \", str(window_size) + \".\" )\n",
    "        maxx_tmp = []\n",
    "        maxy_tmp = []\n",
    "        maxz_tmp = []\n",
    "        mean_tmp = []\n",
    "        std_tmp = []        \n",
    "        for i in range(0, num_features, window_shift):\n",
    "            windowx = values[i:window_size + i, 0]\n",
    "            windowy = values[i:window_size + i, 1]\n",
    "            windowz = values[i:window_size + i, 2]\n",
    "                        \n",
    "            meanx = np.mean(windowx)\n",
    "            meany = np.mean(windowy)\n",
    "            meanz = np.mean(windowz)\n",
    "            mean_tmp.append( (meanx + meany + meanz) )\n",
    "\n",
    "            stdx = np.std(windowx)\n",
    "            stdy = np.std(windowy)\n",
    "            stdz = np.std(windowz)\n",
    "            std_tmp.append( (stdx + stdy + stdz) )\n",
    "            \n",
    "            maxx_tmp.append(np.amax(windowx))\n",
    "            maxy_tmp.append(np.amax(windowy))\n",
    "            maxz_tmp.append(np.amax(windowz))\n",
    "\n",
    "        features = {}\n",
    "        features['mean'] = mean_tmp\n",
    "        features['std'] =  std_tmp\n",
    "        features['maxx'] = maxx_tmp\n",
    "        features['maxy'] = maxy_tmp\n",
    "        features['maxz'] = maxz_tmp\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def compute_features(self, subjects=SUBJECTS, data=BASELINE_DATA, window_size=42000, window_shift=175):\n",
    "        \"\"\" \n",
    "        Calculates features for the provided subjects given the data and\n",
    "        window size/shift of interest. \n",
    "        \n",
    "        Parameters:\n",
    "        subjects (list): subject ids to be loaded\n",
    "        data (list): List of dictionaries containing subjects as indices\n",
    "                     and dictionaries with features for each 'ACC', 'EDA', 'Temp'\n",
    "        window_size (int): specifies size of the sliding window\n",
    "        window_shift (int): Specifies the sliding window shift\n",
    "        Returns:\n",
    "            \n",
    "        \"\"\"\n",
    "        keys = list(DataManager.FEATURES.keys())\n",
    "        print('Computing features..')\n",
    "        for subject in subjects:\n",
    "            print(\"\\tsubject:\", subject)\n",
    "            index = subject - 2\n",
    "            key_index = 0\n",
    "            \n",
    "            acc = self.get_features_for_acc(data[index]['ACC'], window_size, window_shift)\n",
    "            for feature in DataManager.FEATURE_ACC_KEYS:\n",
    "                #print('computed ', len(acc[feature]), 'windows for acc ', feature)\n",
    "                DataManager.FEATURES[keys[key_index]].extend(acc[feature])\n",
    "                key_index = key_index + 1\n",
    "            \n",
    "            eda = self.get_stats(data[index]['EDA'], window_size, window_shift)\n",
    "            for feature in DataManager.FEATURE_KEYS:\n",
    "                #print('computed ', len(eda[feature]), 'windows for eda ', feature)\n",
    "                DataManager.FEATURES[keys[key_index]].extend(eda[feature])\n",
    "                key_index = key_index + 1\n",
    "\n",
    "            temp = self.get_stats(data[index]['TEMP'], window_size, window_shift)\n",
    "            for feature in DataManager.FEATURE_KEYS:\n",
    "                #print('computed ', len(temp[feature]), 'windows for temp ', feature)\n",
    "                DataManager.FEATURES[keys[key_index]].extend(temp[feature])\n",
    "                key_index = key_index + 1\n",
    "\n",
    "           \n",
    "\n",
    "            bvp = self.get_stats(data[index]['BVP'],window_size,window_shift)\n",
    "            for feature in DataManager.FEATURE_KEYS:\n",
    "                #print('computed',len(bvp[feature]),'windows for bvp'feature)\n",
    "                DataManager.FEATURES[keys[key_index]].extend(bvp[feature])\n",
    "                key_index = key_index + 1\n",
    "\n",
    "            ecg = self.get_stats(data[index]['ECG'],window_size,window_shift)\n",
    "            for feature in DataManager.FEATURE_KEYS:\n",
    "                #print('computed',len(ecg[feature]), 'windows for ecg', feature)\n",
    "                DataManager.FEATURES[keys[key_index]].extend(ecg[feature])\n",
    "                key_index = key_index + 1\n",
    "\n",
    "            \n",
    "        return DataManager.FEATURES\n",
    "\n",
    "    def compute_features_stress(self, subjects=SUBJECTS, data=STRESS_DATA, window_size=42000, window_shift=175):\n",
    "        keys = list(DataManager.STRESS_FEATURES.keys())\n",
    "        print('computing features..')    \n",
    "        for subject in subjects:\n",
    "            print(\"\\tsubject:\", subject)\n",
    "            index = subject - 2\n",
    "            key_index = 0\n",
    "            \n",
    "            acc = self.get_features_for_acc(data[index]['ACC'], window_size, window_shift)\n",
    "            for feature in DataManager.FEATURE_ACC_KEYS:\n",
    "                #print('computed ', len(acc[feature]), 'windows for acc ', feature)\n",
    "                DataManager.STRESS_FEATURES[keys[key_index]].extend(acc[feature])\n",
    "                key_index = key_index + 1\n",
    "            \n",
    "            eda = self.get_stats(data[index]['EDA'], window_size, window_shift)\n",
    "            for feature in DataManager.FEATURE_KEYS:\n",
    "                #print('computed ', len(eda[feature]), 'windows for eda ', feature)\n",
    "                DataManager.STRESS_FEATURES[keys[key_index]].extend(eda[feature])\n",
    "                key_index = key_index + 1\n",
    "\n",
    "            temp = self.get_stats(data[index]['TEMP'], window_size, window_shift)\n",
    "            for feature in DataManager.FEATURE_KEYS:\n",
    "                #print('computed ', len(temp[feature]), 'windows for temp ', feature)\n",
    "                DataManager.STRESS_FEATURES[keys[key_index]].extend(temp[feature])\n",
    "                key_index = key_index + 1\n",
    "\n",
    "            \n",
    "\n",
    "            bvp = self.get_stats(data[index]['BVP'],window_size,window_shift)\n",
    "            for feature in DataManager.FEATURE_KEYS:\n",
    "                #print statement\n",
    "                DataManager.STRESS_FEATURES[keys[key_index]].extend(bvp[feature])\n",
    "                key_index = key_index + 1\n",
    "\n",
    "            ecg = self.get_stats(data[index]['ECG'],window_size, window_shift)\n",
    "            for feature in DataManager.FEATURE_KEYS:\n",
    "                #print statement\n",
    "                DataManager.STRESS_FEATURES[keys[key_index]].extend(ecg[feature])\n",
    "                key_index = key_index + 1\n",
    "\n",
    "        return DataManager.STRESS_FEATURES\n",
    "\n",
    "    def get_train_and_test_data(self):\n",
    "        X1 = []\n",
    "        X2 = []\n",
    "        for i in range(0, len(DataManager.FEATURES['a_mean'])):\n",
    "            X1.append([DataManager.FEATURES['a_mean'][i], DataManager.FEATURES['a_std'][i],\\\n",
    "                       DataManager.FEATURES['a_maxx'][i], DataManager.FEATURES['a_maxy'][i],\\\n",
    "                       DataManager.FEATURES['a_maxz'][i], DataManager.FEATURES['e_max'][i],\\\n",
    "                       DataManager.FEATURES['e_min'][i],  DataManager.FEATURES['e_mean'][i],\\\n",
    "                       DataManager.FEATURES['e_range'][i],DataManager.FEATURES['e_std'][i],\\\n",
    "                       DataManager.FEATURES['t_max'][i],  DataManager.FEATURES['t_min'][i],\\\n",
    "                       DataManager.FEATURES['t_mean'][i], DataManager.FEATURES['t_range'][i],\\\n",
    "                       DataManager.FEATURES['t_std'][i], DataManager.FEATURES['b_max'][i],  DataManager.FEATURES['b_min'][i],\\\n",
    "                       DataManager.FEATURES['b_mean'][i], DataManager.FEATURES['b_range'][i],\\\n",
    "                       DataManager.FEATURES['b_std'][i], DataManager.FEATURES['ecg_max'][i],  DataManager.FEATURES['ecg_min'][i],\\\n",
    "                       DataManager.FEATURES['ecg_mean'][i], DataManager.FEATURES['ecg_range'][i],\\\n",
    "                       DataManager.FEATURES['ecg_std'][i]])\n",
    "        #print(np.shape(X1))\n",
    "        \n",
    "        for i in range(0,  len(DataManager.STRESS_FEATURES['a_mean'])):\n",
    "            X2.append([DataManager.FEATURES['a_mean'][i], DataManager.FEATURES['a_std'][i],\\\n",
    "                       DataManager.FEATURES['a_maxx'][i], DataManager.FEATURES['a_maxy'][i],\\\n",
    "                       DataManager.FEATURES['a_maxz'][i], DataManager.FEATURES['e_max'][i],\\\n",
    "                       DataManager.FEATURES['e_min'][i],  DataManager.FEATURES['e_mean'][i],\\\n",
    "                       DataManager.FEATURES['e_range'][i],DataManager.FEATURES['e_std'][i],\\\n",
    "                       DataManager.FEATURES['t_max'][i],  DataManager.FEATURES['t_min'][i],\\\n",
    "                       DataManager.FEATURES['t_mean'][i], DataManager.FEATURES['t_range'][i],\\\n",
    "                       DataManager.FEATURES['t_std'][i], DataManager.FEATURES['b_max'][i],  DataManager.FEATURES['b_min'][i],\\\n",
    "                       DataManager.FEATURES['b_mean'][i], DataManager.FEATURES['b_range'][i],\\\n",
    "                       DataManager.FEATURES['b_std'][i], DataManager.FEATURES['ecg_max'][i],  DataManager.FEATURES['ecg_min'][i],\\\n",
    "                       DataManager.FEATURES['ecg_mean'][i], DataManager.FEATURES['ecg_range'][i],\\\n",
    "                       DataManager.FEATURES['ecg_std'][i]])                \n",
    "        #print(np.shape(X2))\n",
    "        \n",
    "        # initialize zero for base and 1 for stress\n",
    "        y1 = [0] * len(X1)\n",
    "        y2 = [1] * len(X2)\n",
    "        # Now we need to concat the data between baseline and stress so that \n",
    "        # we can split it into training and test sets    \n",
    "        X = np.concatenate((X1, X2), axis=0)\n",
    "        #print(np.shape(X))\n",
    "        \n",
    "        y = np.concatenate((y1,y2), axis=0)\n",
    "        #print(np.shape(y))\n",
    "        #X_train, X_test, y_train, y_test = \\\n",
    "            #train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "        return (X, y)\n",
    "\n",
    "    def normalize(self, data):\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        return scaler.fit_transform(data)\n",
    "\n",
    "    def scale_data(self, X_train, X_test, y_train, y_test):\n",
    "        print(\"Scaling the data...\")\n",
    "        X, y = self.get_train_and_test_data()\n",
    "        (X_train, X_test, y_train, y_test) = (X,X,y,y)\n",
    "        X_train = self.normalize(X_train)\n",
    "        X_test = self.normalize(X_test)\n",
    "        X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "        X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "        return (X_train, X_test, y_train, y_test)\n",
    "\n",
    "    def build_model(self):\n",
    "        num_neurons = 20\n",
    "        num_features = 20\n",
    "        \n",
    "        print('Building the LSTM NN...')\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(30,input_shape=(1, num_features),activation = 'relu'))\n",
    "        #model.add(LSTM(num_neurons, input_shape=(1, num_features), return_sequences=True))\n",
    "        model.add(LSTM(num_neurons, input_shape=(1, num_features), return_sequences=False))\n",
    "        model.add(Dense(30, activation='sigmoid'))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        model = self.configure_learning(model)\n",
    "        print(model.summary())\n",
    "        return model\n",
    "\n",
    "    def configure_learning(self, model):\n",
    "        opt = SGD(lr=0.05)\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt,\\\n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def train_model(self, model, X_train, X_test, y_train, y_test,\\\n",
    "                    batch_size=32, epochs=5):\n",
    "        print('Training network...')\n",
    "        model.fit(X_train, y_train,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  validation_data=(X_test, y_test))\n",
    "        #print(\"inputs: \" , model.input_shape)\n",
    "        #print(\"outputs: \", model.output_shape)\n",
    "        #print(\"actual inputs: \", np.shape(X_train))\n",
    "        #print(\"actual outputs: \", np.shape(y_train))\n",
    "        score, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "        return (model, score, acc)\n",
    "\n",
    "    def load_model(self, file_name=last_saved):\n",
    "        # Load the model of interest\n",
    "        print(\"Loading model:\", file_name)\n",
    "        file = (os.path.join(DataManager.MODELS_DIR, file_name))\n",
    "        model_from_disc = load_model(file)\n",
    "        return model_from_disc\n",
    "\n",
    "    def save_model(self, model):\n",
    "        now = datetime.datetime.now()\n",
    "        # Make sure the datetime str has no special characters and no spaces\n",
    "        DataManager.last_saved = str(\"model-\" + \\\n",
    "                                     str(now.replace(microsecond=0)) +\\\n",
    "                                     \".h5\").replace(\" \", \"\").replace(\":\", \"_\")\n",
    "        model.save(os.path.join(DataManager.MODELS_DIR, DataManager.last_saved))\n",
    "        print(\"Saved model to disc:\",\\\n",
    "              DataManager.last_saved)\n",
    "        \n",
    "   \n",
    "\n",
    "    def create_network(self, epochs=5, batch_size=32):\n",
    "        '''\n",
    "        Builds, trains, and saves a model with the provided number of epochs\n",
    "        and batch size.\n",
    "        '''\n",
    "        (X_train, X_test, y_train, y_test) = self.get_train_and_test_data()\n",
    "        (X_train, X_test, y_train, y_test) = \\\n",
    "            self.scale_data(X_train, X_test, y_train, y_test)\n",
    "        model = self.build_model()\n",
    "        (model, score, acc) = self.train_model(model, X_train, X_test, y_train, y_test,\\\n",
    "                                 batch_size, epochs)\n",
    "        self.save_model(model)\n",
    "        \n",
    "        #self.get_model_results(model, X_train, X_test, y_train, y_test)\n",
    "        return (model, X_train, X_test, y_train, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "6sqboLDJT2aW"
   },
   "outputs": [],
   "source": [
    "\n",
    "  \n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import scipy.signal as scisig\n",
    "import scipy.stats\n",
    "#import cvxEDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "DTO7Rk37WrXN"
   },
   "outputs": [],
   "source": [
    "manager = DataManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EM5N2AcFWvSm",
    "outputId": "c9844f34-6041-405b-8cfe-90807f4f6d2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for S2\n",
      "Loading data for S3\n",
      "Loading data for S4\n",
      "Loading data for S5\n",
      "Loading data for S6\n",
      "Loading data for S7\n",
      "Loading data for S8\n",
      "Loading data for S9\n",
      "Loading data for S10\n",
      "Loading data for S11\n"
     ]
    }
   ],
   "source": [
    "manager.load_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C1kfgPSoWw-U",
    "outputId": "ed42dc9a-13b7-4a91-9273-79d6c4829cd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing features..\n",
      "\tsubject: 2\n",
      "\tsubject: 3\n",
      "\tsubject: 4\n",
      "\tsubject: 5\n",
      "\tsubject: 6\n",
      "\tsubject: 7\n",
      "\tsubject: 8\n",
      "\tsubject: 9\n",
      "\tsubject: 10\n",
      "\tsubject: 11\n",
      "computing features..\n",
      "\tsubject: 2\n",
      "\tsubject: 3\n",
      "\tsubject: 4\n",
      "\tsubject: 5\n",
      "\tsubject: 6\n",
      "\tsubject: 7\n",
      "\tsubject: 8\n",
      "\tsubject: 9\n",
      "\tsubject: 10\n",
      "\tsubject: 11\n"
     ]
    }
   ],
   "source": [
    "manager.compute_features();\n",
    "manager.compute_features_stress();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "c61gOVg6WyoH"
   },
   "outputs": [],
   "source": [
    "X,y = manager.get_train_and_test_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0b1UcaRoW2Br",
    "outputId": "2ec33fc2-2700-47c0-a339-51e4c090db11"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68241, 25)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "V9TrvK0wYKtj"
   },
   "outputs": [],
   "source": [
    "#Removing ECG as it is not sampled from Chest\n",
    "#Using same values for now\n",
    "XX_train = X[:,:-5]\n",
    "XX_test = X[:,:-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "urT2yP4-YS_P",
    "outputId": "2d99c5cf-0e69-41dd-eca1-9257f7f58848"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68241, 20)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(XX_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "XyPTJSEbihBT"
   },
   "outputs": [],
   "source": [
    "y_train = y #Using same values for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "1BvLzEvfii-l"
   },
   "outputs": [],
   "source": [
    "y_test = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TnKRejIJYjb8",
    "outputId": "736f289d-507f-4d8a-8d4f-7b786a558a84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling the data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[[0.21796381, 0.17525588, 0.19851427, ..., 0.73404255,\n",
       "          0.84202719, 0.91174985]],\n",
       " \n",
       "        [[0.2172404 , 0.17525588, 0.1981536 , ..., 0.73404255,\n",
       "          0.8417597 , 0.91177292]],\n",
       " \n",
       "        [[0.21633604, 0.17525588, 0.19779596, ..., 0.73404255,\n",
       "          0.84142022, 0.91178664]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.66419259, 0.83635078, 0.76843004, ..., 0.4893617 ,\n",
       "          0.80212298, 0.19656905]],\n",
       " \n",
       "        [[0.66419259, 0.83635078, 0.7683641 , ..., 0.4893617 ,\n",
       "          0.80227458, 0.1913193 ]],\n",
       " \n",
       "        [[0.66419259, 0.83635078, 0.76829783, ..., 0.4893617 ,\n",
       "          0.80251936, 0.18688425]]]),\n",
       " array([[[0.21796381, 0.17525588, 0.19851427, ..., 0.73404255,\n",
       "          0.84202719, 0.91174985]],\n",
       " \n",
       "        [[0.2172404 , 0.17525588, 0.1981536 , ..., 0.73404255,\n",
       "          0.8417597 , 0.91177292]],\n",
       " \n",
       "        [[0.21633604, 0.17525588, 0.19779596, ..., 0.73404255,\n",
       "          0.84142022, 0.91178664]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.66419259, 0.83635078, 0.76843004, ..., 0.4893617 ,\n",
       "          0.80212298, 0.19656905]],\n",
       " \n",
       "        [[0.66419259, 0.83635078, 0.7683641 , ..., 0.4893617 ,\n",
       "          0.80227458, 0.1913193 ]],\n",
       " \n",
       "        [[0.66419259, 0.83635078, 0.76829783, ..., 0.4893617 ,\n",
       "          0.80251936, 0.18688425]]]),\n",
       " array([0, 0, 0, ..., 1, 1, 1]),\n",
       " array([0, 0, 0, ..., 1, 1, 1]))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.scale_data(XX_train, XX_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "b4AOMEv0Yz1j"
   },
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "XX_train = np.reshape(XX_train, (XX_train.shape[0], 1, XX_train.shape[1]))\n",
    "XX_test = np.reshape(XX_test, (XX_test.shape[0], 1, XX_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mPFibv5eY15t",
    "outputId": "5118dab9-8ccc-43a4-ff54-3b4d6f4ca832"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the LSTM NN...\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_11 (Dense)            (None, 1, 30)             630       \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 20)                4080      \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 30)                630       \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,371\n",
      "Trainable params: 5,371\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training network...\n",
      "Epoch 1/5\n",
      "   1/1067 [..............................] - ETA: 53s - loss: 0.7990 - accuracy: 0.3750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/structured_function.py:265: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  \"Even though the `tf.config.experimental_run_functions_eagerly` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1067/1067 [==============================] - 40s 37ms/step - loss: 0.6437 - accuracy: 0.6504 - val_loss: 0.6415 - val_accuracy: 0.6515\n",
      "Epoch 2/5\n",
      "1067/1067 [==============================] - 45s 42ms/step - loss: 0.6396 - accuracy: 0.6515 - val_loss: 0.6380 - val_accuracy: 0.6515\n",
      "Epoch 3/5\n",
      "1067/1067 [==============================] - 45s 42ms/step - loss: 0.6372 - accuracy: 0.6515 - val_loss: 0.6402 - val_accuracy: 0.6515\n",
      "Epoch 4/5\n",
      "1067/1067 [==============================] - 45s 42ms/step - loss: 0.6369 - accuracy: 0.6517 - val_loss: 0.6358 - val_accuracy: 0.6515\n",
      "Epoch 5/5\n",
      "1067/1067 [==============================] - 45s 42ms/step - loss: 0.6346 - accuracy: 0.6512 - val_loss: 0.6306 - val_accuracy: 0.6515\n",
      "1067/1067 [==============================] - 16s 15ms/step - loss: 0.6306 - accuracy: 0.6515\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<keras.engine.sequential.Sequential at 0x7f545dbe2710>,\n",
       " 0.6306315064430237,\n",
       " 0.6515291333198547)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = manager.build_model()\n",
    "\n",
    "manager.train_model(model, XX_train, XX_test, y_train, y_test,\\\n",
    "                    batch_size=64, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfKv1KS5eKlJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Stress Final .ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
